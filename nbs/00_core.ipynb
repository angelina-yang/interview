{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core ML Competency\n",
    "\n",
    "\n",
    "> In this section..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "> When fine-tuning BERT (Bidirectional Encoder Representations from Transformers) for your use case, what can go wrong? Or what should you pay attention to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![[Source](https://towardsdatascience.com/what-exactly-happens-when-we-fine-tune-bert-f5dc32885d76): Illustration of the pre-training / fine-tuning approach. 3 different downstream NLP tasks, MNLI, NER, and SQuAD, are all solved with the same pre-trained language model, by fine-tuning on the specific task. Image credit: Devlin et al 2019.](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fd250d5-6e90-431c-ba97-b92759f17f9b_764x326.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n",
    "> Fine-tuning a pre-trained language model such as BERT for a specific use case can be a complex process that requires careful consideration and testing to ensure optimal results. Here are some potential issues that can arise during fine-tuning:\n",
    "\n",
    "1. Not using WordPiece tokenizer (You need to use the same tokenizer as the one that was used to train the original model.)\n",
    "\n",
    "2. While fine-tuning, some runs produce degenerate results (This could be task-specific and hyper-parameter tuning is really important.)\n",
    "\n",
    "3. Training new models instead of using pre-trained ones (Too expensive!)\n",
    "\n",
    "Other common issues could be overfitting, bad data quality, cross language training etc.\n",
    "[Dr. Rachael Tatman](https://www.rctatman.com/) from [RASA](https://rasa.com/) specifically pointed out the above three and I think very much worth noting.\n",
    "\n",
    "Let's check out how she explains it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "# these dont work\n",
    "# <video width =\"640\" height =\"480\" controls>\n",
    "#     <source src=\"https://www.youtube.com/clip/UgkxWJGb7jA8peEAHgXBMiY5bnZoC88DX1--\" type=\"video/webm\" >\n",
    "#     </video>\n",
    "    \n",
    "# <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/zMxvS7hD-Ug\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
    "# <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/clip/UgkxWJGb7jA8peEAHgXBMiY5bnZoC88DX1--\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### show video of the answer\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/zMxvS7hD-Ug?clip=UgkxWJGb7jA8peEAHgXBMiY5bnZoC88DX1--&amp;clipt=ELygEBii1xI\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Good Reads on This Topic\n",
    "\n",
    "[Medium blog](https://towardsdatascience.com/what-exactly-happens-when-we-fine-tune-bert-f5dc32885d76). What exactly happens when we fine-tune BERT? Tensorflow [code](https://www.tensorflow.org/tfmodels/nlp/fine_tune_bert) for fine-tuning BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# def foo(): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
